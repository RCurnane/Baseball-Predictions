{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ec13e8-55bb-4ae7-b898-c407d4e5a355",
   "metadata": {},
   "source": [
    "## 1. Importing Required Libraries\n",
    "We begin by importing the necessary Python libraries for data manipulation, visualization, and machine learning. These include:\n",
    "- `pandas` for data handling,\n",
    "- `random` and `numpy` for generating random numbers and working with arrays,\n",
    "- `matplotlib.pyplot` and `seaborn` for data visualization,\n",
    "- `xgboost` for implementing the XGBoost algorithm,\n",
    "- `sklearn` for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e667a99f-e364-4cee-8987-3717648304d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef27617-354d-477a-925b-3a098dead7d3",
   "metadata": {},
   "source": [
    "## 2. Setting Global Seeds\n",
    "Global seeds for both NumPy and Python’s random module are set to ensure reproducibility of results across different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0353a0-cf78-47a4-ab7b-24d752252aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091a2974-af1a-46b6-92ac-4b89d784fdf5",
   "metadata": {},
   "source": [
    "## 3. Loading / Insepecting Data \n",
    "The dataset (Appearances_Percentage_Batter.csv) is loaded into a Pandas DataFrame. This allows us to easily manipulate and analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2981aa83-e3a1-4d3b-b89c-06f30d9ed12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into pandas dataframe\n",
    "%cd \"C:\\Users\\curna\\Desktop\\Data\"\n",
    "df = pd.read_csv('Appearances_Percentage_Batter.csv')\n",
    "# Display first few rows to understand dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c480707-b7ae-47c9-93e0-28accdf4cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check columns for null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0caeb53-b709-44f2-8296-d56eb460ab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the describe method to get high level understanding of data\n",
    "#df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1226484-78a9-4d15-b99a-c4a829f410b8",
   "metadata": {},
   "source": [
    "## 4. Visualizing the Relationship between Plate Appearances (PA) and On-Base Percentage (OBP)\n",
    "To better understand how Plate Appearances (PA) relate to On-Base Percentage (OBP) over different years, we generate a scatter plot. This will allow us to visually assess any trends or correlations.\n",
    "\n",
    "### Steps:\n",
    "1. A list of years is defined (`['16', '17', '18', '19', '20', '21']`) to represent the years of interest.\n",
    "2. A `figure` object is created with a size of 12x8 to ensure the plot has adequate space.\n",
    "3. For each year, a scatter plot is created using `PA_{year}` on the x-axis and `OBP_{year}` on the y-axis. This plots Plate Appearances against On-Base Percentage for that specific year.\n",
    "4. Labels for the x-axis, y-axis, and a title are added for clarity. A legend is also included to differentiate between years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c560ad-f0b9-4e5e-9bc2-2e86a08becd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = ['16', '17', '18', '19', '20', '21']\n",
    "# Define custom colors for each year\n",
    "colors = ['blue', 'green', 'red', 'orange', 'purple', 'gray']\n",
    "\n",
    "# Create figure object and set figure size\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Loop through each year and plot with specific color\n",
    "for year, color in zip(years, colors):\n",
    "    plt.scatter(df[f'PA_{year}'], df[f'OBP_{year}'], label=f'20{year}', color=color)\n",
    "\n",
    "plt.xticks(np.arange(0, 800, 50))\n",
    "\n",
    "# Add title, legend, and x, y labels\n",
    "plt.xlabel('Plate Appearances (PA)')\n",
    "plt.ylabel('On-Base Percentage (OBP)')\n",
    "plt.title('Plate Appearances vs. On-Base Percentage')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718eff2f-ba4d-40b1-a1f3-28150e8dce69",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning and Preparation\n",
    "In this section, we focus on cleaning the dataset by filtering out players with insufficient data and handling missing values.\n",
    "\n",
    "### Steps:\n",
    "1. **Filter Players by Plate Appearances:**  \n",
    "   We filter the DataFrame to retain only those players who have a total of 150 or more plate appearances across the seasons from 2016 to 2020. This step ensures that we analyze players with sufficient data, which is critical for meaningful predictions.\n",
    "\n",
    "2. **Handle Missing Values:**  \n",
    "   For each year (2016 to 2021), we replace any missing values in the On-Base Percentage (OBP) and Plate Appearances (PA) columns with the median of their respective columns. This method of imputation is chosen to maintain the integrity of the dataset while minimizing the influence of outliers.\n",
    "\n",
    "3. **Calculate Age:**  \n",
    "   The player's age in 2021 is computed by subtracting the birth year (extracted from the 'birth_date' column) from 2021. This new 'age' column provides additional information that may be useful in predicting performance metrics.\n",
    "\n",
    "These preprocessing steps are essential for ensuring that the dataset is robust and ready for further analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2763e784-5650-4556-9849-ddb0248aa8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out players with fewer than 150 total plate appearances across multiple seasons\n",
    "df = df[df[['PA_16', 'PA_17', 'PA_18', 'PA_19', 'PA_20']].sum(axis=1) >= 150]\n",
    "\n",
    "# Replace missing values in both OBP and PA columns with their respective medians\n",
    "years = ['16', '17', '18', '19', '20', '21']\n",
    "for year in years:\n",
    "    df[f'OBP_{year}'] = df[f'OBP_{year}'].fillna(df[f'OBP_{year}'].median())\n",
    "    df[f'PA_{year}'] = df[f'PA_{year}'].fillna(df[f'PA_{year}'].median())\n",
    "\n",
    "# Calculate player's age in 2021 by subtracting birth year from 2021\n",
    "df['age'] = 2021 - pd.to_datetime(df['birth_date']).dt.year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1359b1a7-ff91-4167-9983-251d7c46f810",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis\r\n",
    "In this section, we analyze the relationships between selected features and the target variable by calculating and visualizing their correlations.\r\n",
    "\r\n",
    "### Steps:\r\n",
    "1. **Select Relevant Columns:**  \r\n",
    "   A list named `data_cols` is created, which includes the relevant feature columns and the target variable for analysis. The columns chosen are:\r\n",
    "   - Plate Appearances (PA) and On-Base Percentage (OBP) for the years 2016 to 2021.\r\n",
    "   - The player's age.  \r\n",
    "   These columns will help in understanding how different metrics relate to one another.\r\n",
    "\r\n",
    "2. **Initialize the Figure:**  \r\n",
    "   A new figure is initialized with a specified size of 10 by 5 inches to provide a clear view of the heatmap.\r\n",
    "\r\n",
    "3. **Calculate the Correlation Matrix:**  \r\n",
    "   The correlation matrix is computed using the `corr()` method on the selected columns. This matrix quantifies the linear relationship between pairs of features, with values ranging from -1 (perfect negative correlation) to 1 (perfect positive correlation).\r\n",
    "\r\n",
    "4. **Generate the Heatmap:**  \r\n",
    "   A heatmap is created using Seaborn’s `heatmap()` function to visualize the correlation matrix. The `annot` parameter is set to `True` to display the correlation coefficients within the heatmap, and the color map `coolwarm` is used to visually represent the strength and direction of correlations.\r\n",
    "\r\n",
    "5. **Set the Title:**  \r\n",
    "   The heatmap is titled 'Correlation Heatmap of All Features' to provide context for the analysis.\r\n",
    "\r\n",
    "This analysis allows us to identify significant correlations between features, which can inform feature selection and modeling strategies in subsequent steps.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eb6ece-4bee-4e4a-bab7-e21037e79b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns to be used as features and target variables\n",
    "data_cols = ['PA_21', 'OBP_21', 'PA_20', 'OBP_20', 'PA_19', 'OBP_19', 'PA_18', 'OBP_18', 'PA_17', 'OBP_17', 'PA_16', 'OBP_16', 'age']\n",
    "\n",
    "# Initialize figure and specify its size\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Calculate the correlation matrix for the chosen features\n",
    "correlation_matrix = df[data_cols].corr()\n",
    "\n",
    "# Generate a heatmap to visualize the correlation matrix\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "\n",
    "# Set the title for the heatmap\n",
    "plt.title('Correlation Heatmap of All Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eccc23-5bbc-4cc5-ac95-0ee6fd722bf1",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering\r\n",
    "In this section, we enhance the dataset by creating new features that better capture player performance trends and the significance of recent statistics.\r\n",
    "\r\n",
    "### Steps:\r\n",
    "1. **Generate a Weighted OBP Metric:**  \r\n",
    "   A new column, `OBP_weighted`, is created to represent a weighted average of on-base percentage (OBP) over the past five years (2016 to 2020). This metric assigns more importance to the most recent seasons:\r\n",
    "   - OBP for 2016 is weighted at 10%\r\n",
    "   - OBP for 2017 is weighted at 15%\r\n",
    "   - OBP for 2018 is weighted at 20%\r\n",
    "   - OBP for 2019 is weighted at 25%\r\n",
    "   - OBP for 2020 is weighted at 30%  \r\n",
    "   This weighted approach helps to emphasize the players' current performance trends, which are more indicative of their future performance.\r\n",
    "\r\n",
    "2. **Calculate OBP Trends:**  \r\n",
    "   Two new columns, `OBP_trend_1920` and `OBP_trend_1819`, are computed to capture the change in OBP between consecutive seasons:\r\n",
    "   - `OBP_trend_1920` reflects the difference between the OBP of 2020 and 2019, indicating how a player’s performance improved or declined from 2019 to 2020.\r\n",
    "   - `OBP_trend_1819` measures the change in OBP from 2018 to 2019.  \r\n",
    "   These trend features provide insights into players' performance consistency and improvement over time, which can be valuable for making predictions.\r\n",
    "\r\n",
    "By incorporating these engineered features, the dataset is better equipped for modeling, potentially leading to improved pedictive accuracy.\r\n",
    "'] * 0.3)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e214e77-8d48-4395-90dd-5999ff2481e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a weighted OBP metric, giving more significance to recent years\n",
    "df['OBP_weighted'] = (df['OBP_16'] * 0.1 + df['OBP_17'] * 0.15 \n",
    "                      + df['OBP_18'] * 0.2 + df['OBP_19'] * 0.25 \n",
    "                      + df['OBP_20'] * 0.3)\n",
    "\n",
    "# Calculate OBP trends to reflect how performance changes between consecutive seasons\n",
    "df['OBP_trend_1920'] = df['OBP_20'] - df['OBP_19']\n",
    "df['OBP_trend_1819'] = df['OBP_19'] - df['OBP_18']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb13577-ce02-435b-b987-946b40d72702",
   "metadata": {},
   "source": [
    "## 8. Data Preparation for Modeling\r\n",
    "In this section, we specify the feature columns to be used for predicting the target variable. The dataset is then split into training and test sets to facilitate model training and evaluation.\r\n",
    "\r\n",
    "### Steps:\r\n",
    "1. **Define Feature Columns and Target Variable:**  \r\n",
    "   A list named `features` is created to specify the independent variables that will be used for prediction. These include various statistics from previous years along with calculated features like `OBP_weighted` and `OBP_trend`. The target variable, `OBP_21`, represents the on-base percentage for the year 2021, which we aim to predict.\r\n",
    "\r\n",
    "2. **Split the Data:**  \r\n",
    "   The dataset is split into training and testing subsets using the `train_test_split` function from the `sklearn.model_selection` module. This function randomly divides the data:\r\n",
    "   - `x_train` and `y_train` are the training features and target values, respectively.\r\n",
    "   - `x_test` and `y_test` are the test features and target values.\r\n",
    "   The `train_size` parameter is set to 0.7, indicating that 70% of the data will be used for training, while 30% will be reserved for testing. The `random_state` parameter is set to 42 to ensure reproducibility of the results across different runs.\r\n",
    "\r\n",
    "By preparing the data in this manner, we can effectively train models and evaluate their performance based on unsen test data.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584db2b1-f8b5-470c-8aaf-612c2d7c5e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the feature columns and the target variable for prediction\n",
    "features = ['PA_20', 'OBP_20', 'PA_19', 'OBP_19', 'PA_18', 'OBP_18', 'PA_17', 'OBP_17', 'PA_16', 'OBP_16',\n",
    "            'age', 'OBP_weighted', 'OBP_trend_1920', 'OBP_trend_1819']\n",
    "target = 'OBP_21'\n",
    "\n",
    "# Split the data into training and test sets, ensuring results can be reproduced\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[features], df[target], train_size=0.7, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a831147b-652f-4b0e-91e8-56492335fdfc",
   "metadata": {},
   "source": [
    "## 9. Model Training\r\n",
    "In this section, we create a variety of regression models to predict on-base percentage (OBP) and train each model using the training dataset. The models include a range of regression techniques to compare their performance.\r\n",
    "\r\n",
    "### Steps:\r\n",
    "1. **Create a Dictionary of Models:**  \r\n",
    "   A dictionary named `models` is established to store different regression models. Each model is associated with its corresponding name for easy reference during evaluation. The models included are:\r\n",
    "   - Linear Regression\r\n",
    "   - Ridge Regression\r\n",
    "   - Lasso Regression\r\n",
    "   - Random Forest Regression\r\n",
    "   - Decision Tree Regressor\r\n",
    "   - Gradient Boosting\r\n",
    "   - AdaBoost\r\n",
    "   - ElasticNet Regression\r\n",
    "   - XGBoost\r\n",
    "\r\n",
    "2. **Train Each Model:**  \r\n",
    "   A loop iterates through the values of the `models` dictionary, which contains the instantiated model objects. For each model, the `fit` method is called using the training features (`x_train`) and target values (`y_train`). This step trains the model on the provided dataset, allowing it to learn the underlying patterns necessary for making predictions.\r\n",
    "\r\n",
    "By training multiple models, we can later compare their performance and select the best one based on predictive accuracy and other valuation metrics.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3b0b86-4c18-4f33-a0fe-cbb87ca7dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of models to be evaluated\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(),\n",
    "    'Lasso Regression': Lasso(),\n",
    "    'Random Forest Regression': RandomForestRegressor(),\n",
    "    'Decision Tree Regressor': DecisionTreeRegressor(),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(),\n",
    "    'AdaBoost': AdaBoostRegressor(),\n",
    "    'ElasticNet Regression': ElasticNet(),\n",
    "    'XGBoost': XGBRegressor()\n",
    "}\n",
    "\n",
    "# Train each model using the training dataset\n",
    "for model in models.values():\n",
    "    model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2938bc-6e5b-43b3-8aab-b86cf1fa58f6",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation\r\n",
    "In this section, we define a function to assess the accuracy of the predictions made by the various regression models. The evaluation focuses on two key performance metrics: Mean Squared Error (MSE) and the Coefficient of Determination (R²).\r\n",
    "\r\n",
    "### Steps:\r\n",
    "1. **Define the Evaluation Function:**  \r\n",
    "   A function named `evaluate_model` is created to evaluate the performance of a given model. The function takes three parameters: the model to be evaluated, the test features (`x_test`), and the actual target values (`y_test`).\r\n",
    "\r\n",
    "2. **Generate Predictions:**  \r\n",
    "   Inside the function, predictions for the test dataset are generated using the provided model. This step is crucial for comparing the predicted values against the actual values.\r\n",
    "\r\n",
    "3. **Compute Mean Squared Error (MSE):**  \r\n",
    "   The Mean Squared Error is calculated by comparing the actual target values with the predicted values. MSE provides a measure of how close the predictions are to the actual outcomes, with lower values indicating better accuracy.\r\n",
    "\r\n",
    "4. **Compute Coefficient of Determination (R²):**  \r\n",
    "   The Coefficient of Determination is computed to assess the model's explanatory power. R² indicates the proportion of variance in the dependent variable that can be explained by the independent variables, with values closer to 1 signifying a better fit.\r\n",
    "\r\n",
    "5. **Display Performance Metrics:**  \r\n",
    "   The MSE and R² values are printed to the console, providing a summary of the model's performance.\r\n",
    "\r\n",
    "6. **Assess Each Model's Performance:**  \r\n",
    "   A loop iterates through all models in the `models` dictionary:\r\n",
    "   - For each model, a message is printed to indicate which model is being evaluated.\r\n",
    "   - The `evaluate_model` function is called for each model, passing the test features and actual target values.\r\n",
    "\r\n",
    "By evaluating the models based on MSE and R², we can determine which model performs best in predicting OBP, providing valuable insights for model seection and improvement.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598339f8-8dd0-4136-b053-8bb3bca5a275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to assess the accuracy of model predictions\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "    # Generate predictions for the test dataset\n",
    "    predictions = model.predict(x_test)\n",
    "    # Compute the Mean Squared Error between actual and predicted values\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    # Compute the Coefficient of Determination to assess model performance\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    print(f'MSE: {mse:.4f} and R2: {r2:.4f}')\n",
    "\n",
    "# Assess each model's performance\n",
    "for name, model in models.items():\n",
    "    print(f'Evaluating {name}:')\n",
    "    evaluate_model(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b210be4d-7df8-4446-aac4-a38ba5eabeaa",
   "metadata": {},
   "source": [
    "## 11. Comparison of Model Predictions\r\n",
    "In this section, we compare the predictions made by the top-performing regression models: ElasticNet, Ridge, and Linear Regression. This comparison visually assesses how well each model predicts the actual on-base percentage (OBP) for the year 2021.\r\n",
    "\r\n",
    "### Steps:\r\n",
    "1. **Select Top-Performing Models:**  \r\n",
    "   A list of the best-performing models is defined, including ElasticNet Regression, Ridge Regression, and Linear Regression. These models will be evaluated based on their prediction capabilities.\r\n",
    "\r\n",
    "2. **Initialize Figure for Plotting:**  \r\n",
    "   A new figure is created for plotting the predictions. The figure's size is set to ensure clarity and readability of the visualizations.\r\n",
    "\r\n",
    "3. **Define Colors for the Models:**  \r\n",
    "   A list of colors is specified for the models to distinguish between them in the scatter plot. Colors help to enhance the visual appeal and facilitate easy comparison.\r\n",
    "\r\n",
    "4. **Iterate Through Selected Models:**  \r\n",
    "   A loop is initiated to go through each model in the selected list:\r\n",
    "   - **Retrieve the Model:** Each model is retrieved from the predefined dictionary of models.\r\n",
    "   - **Generate Predictions:** Predictions are generated using the test dataset for the current model.\r\n",
    "   - **Plot Predictions:** The predictions are added to a scatter plot, with actual OBP values on the x-axis and predicted OBP values on the y-axis, using the specified colors for each model.\r\n",
    "\r\n",
    "5. **Add Reference Line for Perfect Predictions:**  \r\n",
    "   A reference line is plotted to represent perfect predictions, where actual values equal predicted values. This line serves as a benchmark to evaluate the accuracy of the models' predictions.\r\n",
    "\r\n",
    "6. **Set Plot Labels and Title:**  \r\n",
    "   The x-axis and y-axis labels are defined, along with the plot title. A legend is added to identify each model's predictions.\r\n",
    "\r\n",
    "7. **Display the Plot:**  \r\n",
    "   Finally, the plot is displayed, allowing for a visual comparison of how well each model's predictions align with the actual OBP values for the year 2021.\r\n",
    "\r\n",
    "By visually comparing the predictions of the different models, we can assess their performance and identify which model offers the most acurate predictions for OBP.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65edf58-245a-4d9b-9f56-8054f085ed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top-performing models for comparison\n",
    "best_models = ['ElasticNet Regression', 'Ridge Regression', 'Linear Regression']\n",
    "\n",
    "# Initialize the figure for plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Define a list of colors for the models\n",
    "colors = ['blue', 'red', 'green']  # dark brown\n",
    "\n",
    "# Iterate through each of the selected models to plot predictions\n",
    "for i in range(len(best_models)):\n",
    "    # Retrieve the model from the dictionary\n",
    "    model = models[best_models[i]]\n",
    "    # Generate predictions using the test dataset\n",
    "    predictions = model.predict(x_test)\n",
    "    # Add the predictions to the scatter plot with specified colors\n",
    "    plt.scatter(y_test, predictions, label=f'{best_models[i]}', color=colors[i])\n",
    "\n",
    "# Add a reference line for perfect predictions\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='black', linestyle='--')\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlabel('Actual OBP from 2021')\n",
    "plt.ylabel('Predicted OBP from 2021')\n",
    "plt.title('Predicted vs. Actual OBP for 2021')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc53788-3ebd-4b61-87fc-89426bc339b9",
   "metadata": {},
   "source": [
    "## 12. Analyzing Feature Importance in Ridge Regression\r\n",
    "In this section, we analyze the feature importance derived from the Ridge Regression model by examining the coefficients associated with each feature. This analysis helps us understand the impact of various features on the target variable, on-base percentage (OBP).\r\n",
    "\r\n",
    "### Steps:\r\n",
    "1. **Select the Model for Analysis:**  \r\n",
    "   The Ridge Regression model is chosen for analysis. This model has been previously trained and is now utilized to extract insights regarding feature importance.\r\n",
    "\r\n",
    "2. **Retrieve Model Coefficients:**  \r\n",
    "   The coefficients of the trained Ridge Regression model are retrieved. These coefficients represent the weight or importance of each feature in predicting the target variable.\r\n",
    "\r\n",
    "3. **Create a DataFrame for Feature Importance:**  \r\n",
    "   A DataFrame is created to display the importance of each feature. The absolute values of the coefficients are calculated to emphasize significant correlations, whether positive or negative. This step allows us to identify which features have the greatest influence on OBP.\r\n",
    "\r\n",
    "4. **Sort Features by Importance:**  \r\n",
    "   The DataFrame is sorted in descending order based on the absolute values of the coefficients. This sorting enables easy identification of the most important features impacting OBP.\r\n",
    "\r\n",
    "5. **Display Feature Importance:**  \r\n",
    "   The feature importance DataFrame is printed, showcasing each feature alongside its calculated importance. Notably, features with significant negative correlations (e.g., increased age negatively impacting OBP) are highlighted, providing valuable insights for interpretation.\r\n",
    "\r\n",
    "By analyzing the coefficients of the Ridge Regression model, we gain a clearer understanding of how various features contribute to predictions, informing potential strategies for player evaluation ad decision-making.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88161d8a-4cd9-4596-80d4-6a8a69b083f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the Ridge Regression model for analysis\n",
    "model = models['Ridge Regression']\n",
    "\n",
    "# Retrieve the coefficients from the trained model\n",
    "coefficients = model.coef_\n",
    "\n",
    "# Create a dataframe to showcase feature importance\n",
    "# *** Calculate the absolute values of the coefficients to highlight significant negative \n",
    "# correlations (e.g., increased age has a strong negative correlation with OBP) ***\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': [round(abs(c), 6) for c in coefficients]\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a27fea-5baf-457f-aa3d-39ea7afd2244",
   "metadata": {},
   "source": [
    "## 13. Feature Selection Using Recursive Feature Elimination (RFE)\r\n",
    "In this section, we employ Recursive Feature Elimination (RFE) to identify the most important features for predicting the on-base percentage (OBP). This process helps to simplify the model by selecting only the most relevant variables, potentially improving model performance and interpretability.\r\n",
    "\r\n",
    "### Steps:\r\n",
    "1. **Initialize RFE:**  \r\n",
    "   RFE is initialized using the previously trained Ridge Regression model as the estimator. The goal is to select the top 10 features that contribute most significantly to the prediction of the target variable.\r\n",
    "\r\n",
    "2. **Fit the RFE Model to Training Data:**  \r\n",
    "   The RFE model is fitted to the training dataset. During this process, RFE evaluates the importance of each feature by recursively removing the least important ones until only the specified number of top features remains.\r\n",
    "\r\n",
    "3. **Retrieve Selected Features:**  \r\n",
    "   The features selected by RFE are retrieved. This includes the names of the top 10 features that have been identified as most significant for predicting OBP.\r\n",
    "\r\n",
    "4. **Define the Target Variable:**  \r\n",
    "   The target variable for prediction, which is the on-base percentage for the year 2021 (`OBP_21`), is defined. This variable will be used in the subsequent modeling steps.\r\n",
    "\r\n",
    "5. **Split the Data into Training and Testing Subsets:**  \r\n",
    "   The dataset is split into training and testing subsets based on the selected features. This split ensures that 70% of the data is used for training, while 30% is reserved for testing, allowing for reproducible results. \r\n",
    "\r\n",
    "By performing RFE, we focus on the most relevant features, which can lead to improved model efficiency and clearer insights into the factos influencing OBP.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf5753e-d375-46f4-a8a3-7870e55cf2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Recursive Feature Elimination (RFE) using the Ridge model, selecting the top 10 features\n",
    "rfe = RFE(estimator=model, n_features_to_select=10)\n",
    "\n",
    "# Fit the RFE model to the training data\n",
    "rfe.fit(x_train, y_train)\n",
    "\n",
    "# Retrieve the features selected by RFE\n",
    "selected_features = x_train.columns[rfe.support_]\n",
    "print(\"Selected Features:\", selected_features)\n",
    "\n",
    "# Define the target variable for prediction\n",
    "target = 'OBP_21'\n",
    "\n",
    "# Split the data into training and testing subsets (ensuring reproducibility)\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[selected_features], df[target], train_size=0.7, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaeadba-3f6e-40bd-877e-8918f410030d",
   "metadata": {},
   "source": [
    "## 14. Hyperparameter Tuning for Ridge Regression\r\n",
    "In this section, we perform hyperparameter tuning on the Ridge Regression model to determine the optimal value for the alpha parameter. This tuning process helps enhance the model's performance by balancing bias and variance.\r\n",
    "\r\n",
    "### Steps:\r\n",
    "1. **Initialize the Model:**  \r\n",
    "   A new Ridge Regression model is instantiated. This model will be optimized based on the alpha parameter.\r\n",
    "\r\n",
    "2. **Define the Parameter Grid:**  \r\n",
    "   A parameter grid is created to explore a range of alpha values. The alpha parameter is varied on a logarithmic scale from \\(10^{-4}\\) to \\(10^{4}\\), allowing for a comprehensive search for the best regularization strength.\r\n",
    "\r\n",
    "3. **Set Up Grid Search with Cross-Validation:**  \r\n",
    "   GridSearchCV is utilized to systematically evaluate different alpha values through cross-validation. The scoring method used is the negative mean squared error (MSE), which helps identify the alpha that minimizes prediction error.\r\n",
    "\r\n",
    "4. **Fit the Model to Training Data:**  \r\n",
    "   The Ridge Regression model is fitted to the training dataset using the parameter grid. This process trains the model while evaluating each alpha value's performance through cross-validation.\r\n",
    "\r\n",
    "5. **Output the Best Alpha Value:**  \r\n",
    "   The best alpha value identified during the grid search is printed. This value represents the optimal regularization strength for the Ridge Regression model, which can be used for subsequent model fitting and evaluation.\r\n",
    "\r\n",
    "By conducting this hyperparameter tuning, we enhance the model's capability to generalize well to unseen data, thereby improving its predctive performance.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00319990-9555-4048-8584-6af038ca3b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new Ridge Regression model\n",
    "ridge = Ridge()\n",
    "\n",
    "# Define a parameter grid to explore optimal values for alpha\n",
    "param_grid = {'alpha': np.logspace(-5, 5, 50)}  # Values from 0.00001 to 100,000\n",
    "\n",
    "# Use GridSearchCV to find the best alpha value through cross-validation\n",
    "# The scoring method is based on the negative mean squared error\n",
    "ridge = GridSearchCV(estimator=ridge, param_grid=param_grid, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the model to the training data\n",
    "ridge.fit(x_train, y_train)\n",
    "\n",
    "# Output the best alpha value for the Ridge Regression model\n",
    "print(f\"Best alpha for Ridge Regression: {ridge.best_params_['alpha']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8289b73b-8681-44ed-aae2-40ff5afb4cd5",
   "metadata": {},
   "source": [
    "## 15. Optimizing and Evaluating the Ridge Regression Model\r\n",
    "In this section, we instantiate a new Ridge Regression model using the optimal alpha value identified during the hyperparameter tuning process. The model is then fitted to the training data, and its performance is evaluated on the test dataset.\r\n",
    "\r\n",
    "### Steps:\r\n",
    "1. **Instantiate the Model:**  \r\n",
    "   A new Ridge Regression model is created using the optimal alpha value obtained from the previous grid search. This model is designed to minimize prediction error while preventing overfitting.\r\n",
    "\r\n",
    "2. **Fit the Model to Training Data:**  \r\n",
    "   The optimized Ridge Regression model is trained using the training dataset. This process involves learning the relationship between the features and the target variable (OBP) based on the training data.\r\n",
    "\r\n",
    "3. **Evaluate Model Performance:**  \r\n",
    "   The performance of the fitted model is assessed on the test dataset using a dedicated evaluation function. This function typically calculates metrics such as Mean Squared Error (MSE) and R² score to quantify how well the model predicts the on-base percentage for the players in the test set.\r\n",
    "\r\n",
    "By optimizing the model and evaluating its performance, we can gain insights into its predictive capabilities and make informed decisions about potential improvements or futher tuning.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710462f6-9ff8-42b8-9dea-2425fe62b7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new Ridge Regression model with the optimal alpha value\n",
    "best_ridge_model = Ridge(alpha=ridge.best_params_['alpha'])\n",
    "\n",
    "# Fit the optimized model to the training data\n",
    "best_ridge_model.fit(x_train, y_train)\n",
    "\n",
    "# Evaluate the performance of the model on the test data\n",
    "evaluate_model(best_ridge_model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8a2013-6f7e-46c7-93c4-69f44df78724",
   "metadata": {},
   "source": [
    "## 16. Final Predictions of All OBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce897996-f920-4cff-a5e4-f9f067f02291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the best Ridge Regression model to a variable for predictions\n",
    "model = best_ridge_model\n",
    "\n",
    "# Generate predictions using the test dataset\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "# Calculate the absolute differences between actual values and model predictions\n",
    "errors = abs(y_test - predictions)\n",
    "\n",
    "# Create a DataFrame to compare actual and predicted values along with the errors\n",
    "error_df = pd.DataFrame({\n",
    "    'Player': df.loc[x_test.index, 'Name'],\n",
    "    'Actual': np.round(y_test, 3),\n",
    "    'Predicted': np.round(predictions, 3),\n",
    "    'Error': np.round(errors, 3)\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by the 'Error' column\n",
    "closest_predictions = error_df.sort_values(by='Error')\n",
    "\n",
    "# Set display options to show all rows\n",
    "pd.set_option('display.max_rows', None)  # None means unlimited\n",
    "\n",
    "# Print all results\n",
    "print(closest_predictions)\n",
    "\n",
    "# Optionally, reset to default after printing if needed\n",
    "pd.reset_option('display.max_rows')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7feb980-1dc3-4552-bae1-a11e7f44aba4",
   "metadata": {},
   "source": [
    "## 17. Distribution of Errors based on predictions\n",
    "Step 1: Set the figure size\r\n",
    "Set the figure size to 10 inches wide and 6 inches tall.\r\n",
    "\r\n",
    "Step 2: Create a histogram for errors\r\n",
    "Create a histogram for the Error column of the closest_predictions dataset with 29 bins ranging from 0 to 0.145 with an increment of 0.005, and plot the kernel density estimate (KDE).\r\n",
    "\r\n",
    "Step 3: Set title and labels\r\n",
    "Set the title of the histogram to \"Distribution of Prediction Errors\" and add labels for the x-axis and y-axis.\r\n",
    "\r\n",
    "Step 4: Set x-ticks\r\n",
    "Set the x-ticks to show all increments and rotate them by 45 degrees for better readability.\r\n",
    "\r\n",
    "Step 5: Initialize counters for predictions\r\n",
    "Initialize counters for predictions within and outside the 0.045 range.\r\n",
    "\r\n",
    "Step 6: Color bars based on x-axis value\r\n",
    "Get the bar heights and positions, and set the color of each bar based on the x-axis value. If the x-axis value is less than or equal to 0.050, color the bar green; otherwise, color it red.\r\n",
    "\r\n",
    "Step 7: Add text to bars\r\n",
    "Add text to each bar showing the height of the bar.\r\n",
    "\r\n",
    "Step 8: Create a legend\r\n",
    "Create a legend (key) in the top right corner to distinguish between successful and unsuccessful predictions.\r\n",
    "\r\n",
    "Step 9: Show the plot\r\n",
    "Show the tight layout.plot with a tight layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cd0113-b5a3-4a55-95b3-55c1b32f45a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create a histogram for errors with a specified number of bins\n",
    "histogram = sns.histplot(closest_predictions['Error'], bins=np.arange(0, 0.145, 0.005), kde=True, color='blue')\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Distribution of Prediction Errors', fontsize=16)\n",
    "plt.xlabel('Absolute Error', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "\n",
    "# Set x-ticks to show all increments\n",
    "plt.xticks(np.arange(0, 0.145, 0.005), rotation=45)\n",
    "\n",
    "# Initialize counters for predictions within and outside the 0.045 range\n",
    "within_range_count = 0\n",
    "outside_range_count = 0\n",
    "\n",
    "# Get the bar heights and positions, and set the color of each bar based on the x-axis value\n",
    "for bar in histogram.patches:\n",
    "    x_value = bar.get_x() + bar.get_width() / 2  # Get the x-axis value for the bar\n",
    "    if x_value <= 0.050:  # Check if the x-axis value is less than 0.045\n",
    "        bar.set_facecolor('green')\n",
    "        within_range_count += bar.get_height()  # Increment within-range count\n",
    "    else:\n",
    "        bar.set_facecolor('red')\n",
    "        outside_range_count += bar.get_height()  # Increment outside-range count\n",
    "    \n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, height, f'({int(height)})', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Create a legend (key) in the top right corner\n",
    "plt.legend(handles=[plt.Rectangle((0, 0), 1, 1, color='green'), plt.Rectangle((0, 0), 1, 1, color='red')],\n",
    "           labels=['Within 0.050 (successful)', 'Outside 0.050 (unsuccesful)'], loc='upper right', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dff1d9b-4d0e-4030-b89b-fb83a034df30",
   "metadata": {},
   "source": [
    "## 17.1 Distribution of Errors based on predictions\n",
    "Step 1: Set the figure size\n",
    "Set the figure size to 8 inches wide and 6 inches tall.\n",
    "\n",
    "Step 2: Create a bar chart for successful and unsuccessful counts\n",
    "Create a bar chart to compare the counts of successful and unsuccessful predictions.\n",
    "\n",
    "Step 3: Set title and labels\n",
    "Set the title of the bar chart to \"Total Successful and Unsuccessful Predictions\" and add labels for the x-axis and y-axis.\n",
    "\n",
    "Step 4: Add total count above each bar\n",
    "Add the total count above each bar to provide a clear visual representation of the data.\n",
    "\n",
    "Step 5: Show the plot\n",
    "Show the plot with a tight layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45caa6ec-a4c8-48a6-98b1-7d5e99d1a8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create a bar chart for successful and unsuccessful counts\n",
    "plt.bar(['Successful', 'Unsuccessful'], [within_range_count, outside_range_count], color=['green', 'red'])\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Total Successful and Unsuccessful Predictions', fontsize=16)\n",
    "plt.xlabel('Prediction Outcome', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "\n",
    "# Add total count above each bar\n",
    "for i, count in enumerate([within_range_count, outside_range_count]):\n",
    "    plt.text(i, count + 1, f'({int(count)})', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
